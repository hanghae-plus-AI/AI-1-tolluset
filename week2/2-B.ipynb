{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymxatB5WYxlL"
   },
   "source": [
    "# 과제 2-B\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L1XCtuFA2qtS",
    "outputId": "bb6d7916-8bd8-46ea-afb4-9cbeb79191a9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install jupyter_black\n",
    "!pip install sacremoses\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nFw0udeuHK7R"
   },
   "outputs": [],
   "source": [
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fQ4LzQtbHK7R"
   },
   "outputs": [],
   "source": [
    "backend = \"mps\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGrkmFKXYQGc"
   },
   "source": [
    "## Last word prediction dataset 준비\n",
    "    \n",
    "  - ✅ 기존의 IMDB dataset을 그대로 활용합니다.\n",
    "  - ✅ `collate_fn` 함수에 다음 수정사항들을 반영하면 됩니다.\n",
    "      - ✅ Label은 text를 token으로 변환했을 때 마지막에서 두 번째 token의 id로 설정합니다.\n",
    "      - ✅ 입력 data는 마지막 두 token을 제외한 나머지 token들의 list로 설정합니다.\n",
    "  - ✅ `from torch.nn.utils.rnn import pad_sequence`를 import해서 사용하셔도 좋습니다.\n",
    "  - ✅ Truncation은 기존과 똑같이 진행하시면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HOdhoBVA1zcu",
    "outputId": "2d81a053-b825-49be-d054-fd1d7007a7f1",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee3f3068f744c57a90a02b534a51106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdd0ff589b4d49429eeb19beb2646fde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6328699fdb7440e59643ae64dd41a379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62305c7013342af8195daf3d817984e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90d47e44377345bdaa0712c540ab9772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daa94198e479407b83a0e2defd1392b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3123695d1e74586a9c7e1e6e14dfd25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a04ef4241bc4ff6a04f4bacab347d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c0a5fe689e44a82bd12cafe28e6b136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb77baaefe484edc85484f5c325e1462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a21d4c4cf1349c49cad074540b73ccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertTokenizerFast\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "ds = load_dataset(\"stanfordnlp/imdb\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    max_len = 400\n",
    "    inputs, labels = [], []\n",
    "    for row in batch:\n",
    "        tokens = tokenizer(row[\"text\"], truncation=True, max_length=max_len).input_ids\n",
    "\n",
    "        inputs.append(tokens[:-2])\n",
    "        labels.append(tokens[-2])\n",
    "\n",
    "    inputs = pad_sequence(\n",
    "        [torch.tensor(input_ids) for input_ids in inputs],\n",
    "        batch_first=True,\n",
    "        padding_value=tokenizer.pad_token_id,\n",
    "    )\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "    return inputs, labels\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    ds[\"train\"], batch_size=64, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    ds[\"test\"], batch_size=64, shuffle=False, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-FshZcTZBQ2"
   },
   "source": [
    "## Self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MBlMVMZcRAxv"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.wq = nn.Linear(input_dim, d_model)\n",
    "        self.wk = nn.Linear(input_dim, d_model)\n",
    "        self.wv = nn.Linear(input_dim, d_model)\n",
    "        self.dense = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        q, k, v = self.wq(x), self.wk(x), self.wv(x)\n",
    "        score = torch.matmul(\n",
    "            q, k.transpose(-1, -2)\n",
    "        )  # (B, S, D) * (B, D, S) = (B, S, S)\n",
    "        score = score / sqrt(self.d_model)\n",
    "\n",
    "        if mask is not None:\n",
    "            score = score + (mask * -1e9)\n",
    "\n",
    "        score = self.softmax(score)\n",
    "        result = torch.matmul(score, v)\n",
    "        result = self.dense(result)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6uhD-qjn3tiH"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, n_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.depth = d_model // n_heads\n",
    "\n",
    "        self.wq = nn.Linear(input_dim, d_model)\n",
    "        self.wk = nn.Linear(input_dim, d_model)\n",
    "        self.wv = nn.Linear(input_dim, d_model)\n",
    "        self.dense = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        # S, D -> S, H, D' -> H, S, D'\n",
    "        return x.view(batch_size, seq_len, self.n_heads, self.depth).transpose(1, 2)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # H, S, D'\n",
    "        q, k, v = (\n",
    "            self.split_heads(self.wq(x)),\n",
    "            self.split_heads(self.wk(x)),\n",
    "            self.split_heads(self.wv(x)),\n",
    "        )\n",
    "\n",
    "        # (H, S, D') * (H, D', S) -> H, S, S\n",
    "        score = torch.matmul(q, k.transpose(-1, -2)) / sqrt(self.depth)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            score = score + (mask * -1e9)\n",
    "\n",
    "        score = self.softmax(score)\n",
    "        result = torch.matmul(score, v)  # (H, S, S) * (H, S, D') -> H, S, D'\n",
    "\n",
    "        # H, S, D' -> S, H, D' -> S, D\n",
    "        result = result.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        result = self.dense(result)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "VZHPCn9AS5Gp"
   },
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, dff, n_heads, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.dff = dff\n",
    "\n",
    "        self.mha = MultiHeadAttention(input_dim, d_model, n_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, dff), nn.ReLU(), nn.Linear(dff, d_model)\n",
    "        )\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(d_model)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x1 = self.mha(x, mask)\n",
    "        x1 = self.dropout(x1)\n",
    "        x1 = self.layernorm1(x1 + x)\n",
    "\n",
    "        x2 = self.ffn(x1)\n",
    "        x2 = self.dropout(x2)\n",
    "        x2 = self.layernorm2(x2 + x1)\n",
    "\n",
    "        return x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3VYrqTJagS1"
   },
   "source": [
    "## Positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uf_jMQWDUR79",
    "outputId": "041b9f5e-0955-40f7-dd09-9b93ba4ac14e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 400, 256])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(\n",
    "        np.arange(position)[:, None], np.arange(d_model)[None, :], d_model\n",
    "    )\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[None, ...]\n",
    "\n",
    "    return torch.FloatTensor(pos_encoding)\n",
    "\n",
    "\n",
    "max_len = 400\n",
    "print(positional_encoding(max_len, 256).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l09ETINVGhEg"
   },
   "source": [
    "- Loss function 및 classifier output 변경\n",
    "    - ✅ 마지막 token id를 예측하는 것이기 때문에 binary classification이 아닌 일반적인 classification 문제로 바뀝니다. MNIST 과제에서 했던 것 처럼 `nn.CrossEntropy` loss와 `TextClassifier`의 출력 차원을 잘 조정하여 task를 풀 수 있도록 수정하시면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8MaiCGh8TsDH"
   },
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_layers, dff, n_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        self.dff = dff\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = nn.parameter.Parameter(\n",
    "            positional_encoding(max_len, d_model), requires_grad=False\n",
    "        )\n",
    "        self.layers = nn.ModuleList(\n",
    "            [TransformerLayer(d_model, d_model, dff, n_heads) for _ in range(n_layers)]\n",
    "        )\n",
    "        self.classification = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = x == tokenizer.pad_token_id\n",
    "        mask = mask[:, None, :]\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x = x * sqrt(self.d_model)\n",
    "        x = x + self.pos_encoding[:, :seq_len]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        x = x[:, -1]\n",
    "        x = self.classification(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "model = TextClassifier(len(tokenizer), 32, 5, 128, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDq05OlAb2lB"
   },
   "source": [
    "## 학습\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "YHVVsWBPQmnv",
    "outputId": "fe7a1fbe-da81-4eef-89bb-a8348032f036"
   },
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "lr = 0.001\n",
    "model = model.to(backend)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "r88BALxO1zc1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def accuracy(model, dataloader):\n",
    "    cnt = 0\n",
    "    acc = 0\n",
    "\n",
    "    for data in dataloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(backend), labels.to(backend)\n",
    "\n",
    "        preds = model(inputs)\n",
    "        preds = torch.argmax(preds, dim=-1)\n",
    "        # preds = (preds > 0).long()[..., 0]\n",
    "\n",
    "        cnt += labels.shape[0]\n",
    "        acc += (labels == preds).sum().item()\n",
    "\n",
    "    return acc / cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "al_b56TYRILq",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | Train Loss: 937.2393565177917\n",
      "=========> Train acc: 0.593 | Test acc: 0.586\n",
      "Epoch   1 | Train Loss: 906.2029795646667\n",
      "=========> Train acc: 0.594 | Test acc: 0.584\n",
      "Epoch   2 | Train Loss: 872.6696382761002\n",
      "=========> Train acc: 0.600 | Test acc: 0.586\n",
      "Epoch   3 | Train Loss: 842.6990832090378\n",
      "=========> Train acc: 0.605 | Test acc: 0.587\n",
      "Epoch   4 | Train Loss: 807.0154939889908\n",
      "=========> Train acc: 0.614 | Test acc: 0.584\n",
      "Epoch   5 | Train Loss: 774.4406059980392\n",
      "=========> Train acc: 0.626 | Test acc: 0.579\n",
      "Epoch   6 | Train Loss: 738.8853043317795\n",
      "=========> Train acc: 0.641 | Test acc: 0.580\n",
      "Epoch   7 | Train Loss: 701.7582052946091\n",
      "=========> Train acc: 0.659 | Test acc: 0.569\n",
      "Epoch   8 | Train Loss: 666.6521001458168\n",
      "=========> Train acc: 0.680 | Test acc: 0.575\n",
      "Epoch   9 | Train Loss: 629.070295393467\n",
      "=========> Train acc: 0.702 | Test acc: 0.566\n",
      "Epoch  10 | Train Loss: 597.1206686496735\n",
      "=========> Train acc: 0.725 | Test acc: 0.557\n",
      "Epoch  11 | Train Loss: 566.2317045927048\n",
      "=========> Train acc: 0.741 | Test acc: 0.527\n",
      "Epoch  12 | Train Loss: 536.8650785684586\n",
      "=========> Train acc: 0.754 | Test acc: 0.563\n",
      "Epoch  13 | Train Loss: 506.5769509077072\n",
      "=========> Train acc: 0.758 | Test acc: 0.568\n",
      "Epoch  14 | Train Loss: 477.6667025089264\n",
      "=========> Train acc: 0.782 | Test acc: 0.558\n",
      "Epoch  15 | Train Loss: 450.96467554569244\n",
      "=========> Train acc: 0.804 | Test acc: 0.556\n",
      "Epoch  16 | Train Loss: 428.514799952507\n",
      "=========> Train acc: 0.820 | Test acc: 0.519\n",
      "Epoch  17 | Train Loss: 411.7480246126652\n",
      "=========> Train acc: 0.837 | Test acc: 0.525\n",
      "Epoch  18 | Train Loss: 389.3293676972389\n",
      "=========> Train acc: 0.844 | Test acc: 0.538\n",
      "Epoch  19 | Train Loss: 368.1028028726578\n",
      "=========> Train acc: 0.849 | Test acc: 0.531\n",
      "Epoch  20 | Train Loss: 351.9383804798126\n",
      "=========> Train acc: 0.866 | Test acc: 0.546\n",
      "Epoch  21 | Train Loss: 329.60736164450645\n",
      "=========> Train acc: 0.877 | Test acc: 0.510\n",
      "Epoch  22 | Train Loss: 316.7110087275505\n",
      "=========> Train acc: 0.884 | Test acc: 0.499\n",
      "Epoch  23 | Train Loss: 302.13272526860237\n",
      "=========> Train acc: 0.895 | Test acc: 0.526\n",
      "Epoch  24 | Train Loss: 287.32694548368454\n",
      "=========> Train acc: 0.904 | Test acc: 0.500\n",
      "Epoch  25 | Train Loss: 271.57693187892437\n",
      "=========> Train acc: 0.918 | Test acc: 0.494\n",
      "Epoch  26 | Train Loss: 261.2138879299164\n",
      "=========> Train acc: 0.917 | Test acc: 0.505\n",
      "Epoch  27 | Train Loss: 247.62375622987747\n",
      "=========> Train acc: 0.931 | Test acc: 0.525\n",
      "Epoch  28 | Train Loss: 236.82960486412048\n",
      "=========> Train acc: 0.931 | Test acc: 0.499\n",
      "Epoch  29 | Train Loss: 225.5723111629486\n",
      "=========> Train acc: 0.937 | Test acc: 0.502\n",
      "Epoch  30 | Train Loss: 217.87030364573002\n",
      "=========> Train acc: 0.942 | Test acc: 0.498\n",
      "Epoch  31 | Train Loss: 211.56472073495388\n",
      "=========> Train acc: 0.947 | Test acc: 0.519\n",
      "Epoch  32 | Train Loss: 199.79807329177856\n",
      "=========> Train acc: 0.952 | Test acc: 0.519\n",
      "Epoch  33 | Train Loss: 194.31925302743912\n",
      "=========> Train acc: 0.956 | Test acc: 0.510\n",
      "Epoch  34 | Train Loss: 182.75387813150883\n",
      "=========> Train acc: 0.956 | Test acc: 0.521\n",
      "Epoch  35 | Train Loss: 172.9608931541443\n",
      "=========> Train acc: 0.961 | Test acc: 0.514\n",
      "Epoch  36 | Train Loss: 168.26017056405544\n",
      "=========> Train acc: 0.966 | Test acc: 0.507\n",
      "Epoch  37 | Train Loss: 161.99912238121033\n",
      "=========> Train acc: 0.962 | Test acc: 0.512\n",
      "Epoch  38 | Train Loss: 157.0249171629548\n",
      "=========> Train acc: 0.961 | Test acc: 0.524\n",
      "Epoch  39 | Train Loss: 156.329281270504\n",
      "=========> Train acc: 0.970 | Test acc: 0.508\n",
      "Epoch  40 | Train Loss: 144.95384710282087\n",
      "=========> Train acc: 0.974 | Test acc: 0.504\n",
      "Epoch  41 | Train Loss: 139.48010600358248\n",
      "=========> Train acc: 0.978 | Test acc: 0.510\n",
      "Epoch  42 | Train Loss: 133.39768820628524\n",
      "=========> Train acc: 0.975 | Test acc: 0.500\n",
      "Epoch  43 | Train Loss: 132.53017752617598\n",
      "=========> Train acc: 0.979 | Test acc: 0.503\n",
      "Epoch  44 | Train Loss: 124.6303611844778\n",
      "=========> Train acc: 0.983 | Test acc: 0.490\n",
      "Epoch  45 | Train Loss: 124.31954453885555\n",
      "=========> Train acc: 0.986 | Test acc: 0.519\n",
      "Epoch  46 | Train Loss: 118.82317991554737\n",
      "=========> Train acc: 0.986 | Test acc: 0.509\n",
      "Epoch  47 | Train Loss: 111.87699292600155\n",
      "=========> Train acc: 0.982 | Test acc: 0.464\n",
      "Epoch  48 | Train Loss: 110.40946922451258\n",
      "=========> Train acc: 0.986 | Test acc: 0.509\n",
      "Epoch  49 | Train Loss: 108.9118973352015\n",
      "=========> Train acc: 0.987 | Test acc: 0.501\n",
      "CPU times: user 1h 32min 59s, sys: 10min 1s, total: 1h 43min\n",
      "Wall time: 5h 37min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "n_epochs = 50\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0.0\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:\n",
    "        model.zero_grad()\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(backend), labels.to(backend)\n",
    "\n",
    "        preds = model(inputs)\n",
    "\n",
    "        loss = loss_fn(preds, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch:3d} | Train Loss: {total_loss}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        train_acc = accuracy(model, train_loader)\n",
    "        test_acc = accuracy(model, test_loader)\n",
    "        print(f\"=========> Train acc: {train_acc:.3f} | Test acc: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
