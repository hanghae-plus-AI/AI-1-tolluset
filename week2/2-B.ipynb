{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymxatB5WYxlL"
   },
   "source": [
    "# 과제 2-B\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "L1XCtuFA2qtS",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "bb6d7916-8bd8-46ea-afb4-9cbeb79191a9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jupyter_black in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (0.4.0)\n",
      "Requirement already satisfied: black>=21 in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from black[jupyter]>=21->jupyter_black) (24.8.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from black>=21->black[jupyter]>=21->jupyter_black) (8.1.7)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from black>=21->black[jupyter]>=21->jupyter_black) (1.0.0)\n",
      "Requirement already satisfied: packaging>=22.0 in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from black>=21->black[jupyter]>=21->jupyter_black) (24.1)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from black>=21->black[jupyter]>=21->jupyter_black) (0.12.1)\n",
      "Requirement already satisfied: platformdirs>=2 in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from black>=21->black[jupyter]>=21->jupyter_black) (4.3.1)\n",
      "Requirement already satisfied: ipython>=7.8.0 in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from black[jupyter]>=21->jupyter_black) (8.27.0)\n",
      "Requirement already satisfied: tokenize-rt>=3.2.0 in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from black[jupyter]>=21->jupyter_black) (6.0.0)\n",
      "Requirement already satisfied: decorator in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from ipython>=7.8.0->black[jupyter]>=21->jupyter_black) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from ipython>=7.8.0->black[jupyter]>=21->jupyter_black) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from ipython>=7.8.0->black[jupyter]>=21->jupyter_black) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from ipython>=7.8.0->black[jupyter]>=21->jupyter_black) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from ipython>=7.8.0->black[jupyter]>=21->jupyter_black) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from ipython>=7.8.0->black[jupyter]>=21->jupyter_black) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from ipython>=7.8.0->black[jupyter]>=21->jupyter_black) (5.14.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from ipython>=7.8.0->black[jupyter]>=21->jupyter_black) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from jedi>=0.16->ipython>=7.8.0->black[jupyter]>=21->jupyter_black) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from pexpect>4.3->ipython>=7.8.0->black[jupyter]>=21->jupyter_black) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.8.0->black[jupyter]>=21->jupyter_black) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from stack-data->ipython>=7.8.0->black[jupyter]>=21->jupyter_black) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from stack-data->ipython>=7.8.0->black[jupyter]>=21->jupyter_black) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from stack-data->ipython>=7.8.0->black[jupyter]>=21->jupyter_black) (0.2.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from asttokens>=2.1.0->stack-data->ipython>=7.8.0->black[jupyter]>=21->jupyter_black) (1.16.0)\n",
      "Requirement already satisfied: sacremoses in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (0.1.1)\n",
      "Requirement already satisfied: regex in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from sacremoses) (2024.9.11)\n",
      "Requirement already satisfied: click in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from sacremoses) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from sacremoses) (1.4.2)\n",
      "Requirement already satisfied: tqdm in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from sacremoses) (4.66.5)\n",
      "Requirement already satisfied: datasets in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (3.0.0)\n",
      "Requirement already satisfied: filelock in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from datasets) (3.15.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from datasets) (2.1.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from datasets) (0.25.0)\n",
      "Requirement already satisfied: packaging in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.11.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install jupyter_black\n",
    "!pip install sacremoses\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nFw0udeuHK7R"
   },
   "outputs": [],
   "source": [
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fQ4LzQtbHK7R"
   },
   "outputs": [],
   "source": [
    "backend = \"mps\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGrkmFKXYQGc"
   },
   "source": [
    "## Last word prediction dataset 준비\n",
    "    \n",
    "  - ✅ 기존의 IMDB dataset을 그대로 활용합니다.\n",
    "  - ✅ `collate_fn` 함수에 다음 수정사항들을 반영하면 됩니다.\n",
    "      - ✅ Label은 text를 token으로 변환했을 때 마지막 token의 id로 설정합니다.\n",
    "      - ✅ 입력 data는 마지막 token을 제외한 나머지 token들의 list로 설정합니다.\n",
    "  - ✅ `from torch.nn.utils.rnn import pad_sequence`를 import해서 사용하셔도 좋습니다.\n",
    "  - ✅ Truncation은 기존과 똑같이 진행하시면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "HOdhoBVA1zcu",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "2d81a053-b825-49be-d054-fd1d7007a7f1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bh/workspaces/studies/hh-ai/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertTokenizerFast\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "ds = load_dataset(\"stanfordnlp/imdb\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    max_len = 400\n",
    "    inputs, labels = [], []\n",
    "    for row in batch:\n",
    "        tokens = tokenizer(row[\"text\"], truncation=True, max_length=max_len).input_ids\n",
    "\n",
    "        inputs.append(tokens[:-1])\n",
    "        labels.append(tokens[-1])\n",
    "\n",
    "    inputs = pad_sequence(\n",
    "        [torch.tensor(input_ids) for input_ids in inputs],\n",
    "        batch_first=True,\n",
    "        padding_value=tokenizer.pad_token_id,\n",
    "    )\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "    return inputs, labels\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    ds[\"train\"], batch_size=64, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    ds[\"test\"], batch_size=64, shuffle=False, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-FshZcTZBQ2"
   },
   "source": [
    "## Self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MBlMVMZcRAxv"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.wq = nn.Linear(input_dim, d_model)\n",
    "        self.wk = nn.Linear(input_dim, d_model)\n",
    "        self.wv = nn.Linear(input_dim, d_model)\n",
    "        self.dense = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        q, k, v = self.wq(x), self.wk(x), self.wv(x)\n",
    "        score = torch.matmul(\n",
    "            q, k.transpose(-1, -2)\n",
    "        )  # (B, S, D) * (B, D, S) = (B, S, S)\n",
    "        score = score / sqrt(self.d_model)\n",
    "\n",
    "        if mask is not None:\n",
    "            score = score + (mask * -1e9)\n",
    "\n",
    "        score = self.softmax(score)\n",
    "        result = torch.matmul(score, v)\n",
    "        result = self.dense(result)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6uhD-qjn3tiH"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, n_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.depth = d_model // n_heads\n",
    "\n",
    "        self.wq = nn.Linear(input_dim, d_model)\n",
    "        self.wk = nn.Linear(input_dim, d_model)\n",
    "        self.wv = nn.Linear(input_dim, d_model)\n",
    "        self.dense = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        # S, D -> S, H, D' -> H, S, D'\n",
    "        return x.view(batch_size, seq_len, self.n_heads, self.depth).transpose(1, 2)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # H, S, D'\n",
    "        q, k, v = (\n",
    "            self.split_heads(self.wq(x)),\n",
    "            self.split_heads(self.wk(x)),\n",
    "            self.split_heads(self.wv(x)),\n",
    "        )\n",
    "\n",
    "        # (H, S, D') * (H, D', S) -> H, S, S\n",
    "        score = torch.matmul(q, k.transpose(-1, -2)) / sqrt(self.depth)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            score = score + (mask * -1e9)\n",
    "\n",
    "        score = self.softmax(score)\n",
    "        result = torch.matmul(score, v)  # (H, S, S) * (H, S, D') -> H, S, D'\n",
    "\n",
    "        # H, S, D' -> S, H, D' -> S, D\n",
    "        result = result.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        result = self.dense(result)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "VZHPCn9AS5Gp"
   },
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, dff, n_heads, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.dff = dff\n",
    "\n",
    "        self.mha = MultiHeadAttention(input_dim, d_model, n_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, dff), nn.ReLU(), nn.Linear(dff, d_model)\n",
    "        )\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(d_model)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x1 = self.mha(x, mask)\n",
    "        x1 = self.dropout(x1)\n",
    "        x1 = self.layernorm1(x1 + x)\n",
    "\n",
    "        x2 = self.ffn(x1)\n",
    "        x2 = self.dropout(x2)\n",
    "        x2 = self.layernorm2(x2 + x1)\n",
    "\n",
    "        return x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3VYrqTJagS1"
   },
   "source": [
    "## Positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uf_jMQWDUR79",
    "outputId": "041b9f5e-0955-40f7-dd09-9b93ba4ac14e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 400, 256])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(\n",
    "        np.arange(position)[:, None], np.arange(d_model)[None, :], d_model\n",
    "    )\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[None, ...]\n",
    "\n",
    "    return torch.FloatTensor(pos_encoding)\n",
    "\n",
    "\n",
    "max_len = 400\n",
    "print(positional_encoding(max_len, 256).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l09ETINVGhEg"
   },
   "source": [
    "- Loss function 및 classifier output 변경\n",
    "    - ✅ 마지막 token id를 예측하는 것이기 때문에 binary classification이 아닌 일반적인 classification 문제로 바뀝니다. MNIST 과제에서 했던 것 처럼 `nn.CrossEntropy` loss와 `TextClassifier`의 출력 차원을 잘 조정하여 task를 풀 수 있도록 수정하시면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8MaiCGh8TsDH"
   },
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_layers, dff, n_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        self.dff = dff\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = nn.parameter.Parameter(\n",
    "            positional_encoding(max_len, d_model), requires_grad=False\n",
    "        )\n",
    "        self.layers = nn.ModuleList(\n",
    "            [TransformerLayer(d_model, d_model, dff, n_heads) for _ in range(n_layers)]\n",
    "        )\n",
    "        self.classification = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = x == tokenizer.pad_token_id\n",
    "        mask = mask[:, None, :]\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x = x * sqrt(self.d_model)\n",
    "        x = x + self.pos_encoding[:, :seq_len]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        x = x[:, -1]\n",
    "        x = self.classification(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "model = TextClassifier(len(tokenizer), 32, 5, 128, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDq05OlAb2lB"
   },
   "source": [
    "## 학습\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "YHVVsWBPQmnv",
    "outputId": "fe7a1fbe-da81-4eef-89bb-a8348032f036"
   },
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "lr = 0.001\n",
    "model = model.to(backend)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "r88BALxO1zc1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def accuracy(model, dataloader):\n",
    "    cnt = 0\n",
    "    acc = 0\n",
    "\n",
    "    for data in dataloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(backend), labels.to(backend)\n",
    "\n",
    "        preds = model(inputs)\n",
    "        preds = torch.argmax(preds, dim=-1)\n",
    "        # preds = (preds > 0).long()[..., 0]\n",
    "\n",
    "        cnt += labels.shape[0]\n",
    "        acc += (labels == preds).sum().item()\n",
    "\n",
    "    return acc / cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "al_b56TYRILq",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | Train Loss: 434.22149012889713\n",
      "=========> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch   1 | Train Loss: 2.3325765430927277\n",
      "=========> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch   2 | Train Loss: 0.8341400201898068\n",
      "=========> Train acc: 1.000 | Test acc: 1.000\n",
      "CPU times: user 5min 39s, sys: 24.2 s, total: 6min 4s\n",
      "Wall time: 22min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 시간관계상 50 -> 3으로 변경\n",
    "n_epochs = 3\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0.0\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:\n",
    "        model.zero_grad()\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(backend), labels.to(backend)\n",
    "\n",
    "        preds = model(inputs)\n",
    "\n",
    "        loss = loss_fn(preds, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch:3d} | Train Loss: {total_loss}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        train_acc = accuracy(model, train_loader)\n",
    "        test_acc = accuracy(model, test_loader)\n",
    "        print(f\"=========> Train acc: {train_acc:.3f} | Test acc: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
